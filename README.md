# ğŸ“š AI PDF Chatbot with RAG (Retrieval-Augmented Generation)

This is a **Flask-based AI chatbot** that allows users to **upload a PDF document and ask questions about its content**. The system processes the PDF using **Granite Embeddings**, stores embeddings using **FAISS**, and generates responses using **DeepSeek-R1:7B**.

## ğŸš€ Features

- âœ… **Upload and Process PDFs**
- âœ… **Chatbot Interface with Markdown-Formatted Responses**
- âœ… **Granite Embedding for Vector Search**
- âœ… **DeepSeek-R1:7B for Answer Generation**
- âœ… **Beautiful UI with Bootstrap**
- âœ… **Session-Based Retrieval for Multiple Questions**

---

## ğŸ“‚ Project Structure

```
ğŸ“‚ AI_PDF_Chatbot
â”‚â”€â”€ ğŸ“ templates
â”‚   â”œâ”€â”€ index.html  # File Upload Page
â”‚   â”œâ”€â”€ chat.html   # Chat Interface
â”‚
â”‚â”€â”€ ğŸ“ uploads      # Stores Uploaded PDFs (Auto genrated)
â”‚â”€â”€ ğŸ“ vector_db    # Stores FAISS Index for each session (Auto genrated)
â”‚â”€â”€ app.py          # Main Flask Server
â”‚â”€â”€ requirements.txt # List of Dependencies (This is my entire pip freeze results, some of them are not needed)
â”‚â”€â”€ README.md       # Documentation
```

---

## ğŸ”§ Installation

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/alihayajneh/OLLAMA_DEEPSEEK_RAG.git
cd AI_PDF_Chatbot
```

### **2ï¸âƒ£ Create a Virtual Environment (Optional but Recommended)**
```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate     # On Windows
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

## ğŸ—ï¸ Install Ollama Models

Since this chatbot uses **Granite Embedding** for text processing and **DeepSeek-R1:7B** for response generation, you need to install these models using **Ollama**.

### **1ï¸âƒ£ Install Ollama**
First, install Ollama on your system by following the instructions [here](https://ollama.com).

### **2ï¸âƒ£ Pull the Required Models**
Run the following commands to download the required models:

```bash
# Install Granite Embedding Model (278M)
ollama pull granite-embedding:278m

# Install DeepSeek-R1:7B Model
ollama pull deepseek-r1:7b
```

Once installed, you can run the chatbot without issues.
---

## ğŸ¯ Usage

### **1ï¸âƒ£ Start the Flask Server**
```bash
python app.py
```
**ğŸ“Œ Access the web interface at:**  
ğŸ”— [http://127.0.0.1:5000](http://127.0.0.1:5000)

### **2ï¸âƒ£ Upload a PDF**
- Click **"Choose File"** and select a PDF.
- Click **"Upload PDF"** to start processing.

### **3ï¸âƒ£ Chat with the PDF**
- After processing, you can enter questions in the chatbot.
- The bot will return **context-aware answers** extracted from the PDF.

---

## ğŸ› ï¸ API Endpoints

| Method | Endpoint      | Description |
|--------|--------------|-------------|
| `GET`  | `/`          | Renders the upload page |
| `POST` | `/upload`    | Uploads a PDF and processes it |
| `POST` | `/ask`       | Accepts user queries and returns AI-generated answers |

---


## ğŸ“œ License
This project is licensed under the **MIT License**. Feel free to modify and distribute.

---



## ğŸŒŸ Acknowledgments
- **LangChain** for RAG Pipeline
- **FAISS** for Vector Search
- **DeepSeek-R1:7B** for AI-generated responses
- **Flask** for the backend framework
- **Bootstrap** for frontend UI

ğŸ”¹ **Made with â¤ï¸ for AI-powered knowledge retrieval by Dr Ali Hayajneh** ğŸš€ 

